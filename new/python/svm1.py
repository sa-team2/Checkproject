import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
import firebase_admin
from firebase_admin import credentials, firestore
from imblearn.over_sampling import SMOTE
import numpy as np

# ========== 1. åˆå§‹åŒ– Firebase ========== 
cred = credentials.Certificate("../config/dayofftest1-firebase-adminsdk-xfpl4-f64d9dc336.json")
firebase_admin.initialize_app(cred)
db = firestore.client()


model_save_path = "bert"  # è¨­å®šå„²å­˜çš„è³‡æ–™å¤¾åç¨±

# æ¸…ç©ºè³‡æ–™å¤¾å…§å®¹ï¼ˆä½†ä¿ç•™è³‡æ–™å¤¾æœ¬èº«ï¼‰
import os
import shutil
if os.path.exists(model_save_path):
    for filename in os.listdir(model_save_path):
        file_path = os.path.join(model_save_path, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
                print(f"æˆåŠŸåˆªé™¤æª”æ¡ˆï¼š{file_path}")
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
                print(f"æˆåŠŸåˆªé™¤è³‡æ–™å¤¾ï¼š{file_path}")
        except Exception as e:
            print(f"âŒ ç„¡æ³•åˆªé™¤ {file_path}ï¼š{e}")
else:
    os.makedirs(model_save_path)
    print(f"ğŸ“ è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå·²å»ºç«‹ï¼š{model_save_path}")





# ========== 2. å¾ Firestore è®€å–è©é¨™æ•¸æ“š ========== 
def get_data_from_firestore():
    collection_ref = db.collection("FraudDefine")
    docs = collection_ref.stream()
    data, labels = [], []
    for doc in docs:
        doc_data = doc.to_dict()
        data.append(doc_data["Keyword"])
        result = doc_data.get('Result', 0)  # é»˜èªç‚º 0ï¼Œè‹¥ç¼ºå¤±çµæœ
        labels.append(result)  # é€™è£¡éœ€è¦å°‡ Result æ·»åŠ åˆ° labels ä¸­
    print(f"ç²å– {len(data)} æ¢è©é¨™æ•¸æ“š")
    return data, labels

texts, labels = get_data_from_firestore()

# ========== 3. æ–‡æœ¬ç·¨ç¢¼ï¼ˆä½¿ç”¨ BERT Tokenizerï¼‰ ========== 
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

def encode_texts(texts, tokenizer, max_length=32):
    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")

encoded_texts = encode_texts(texts, tokenizer)

# ========== 4. ä½¿ç”¨ SMOTE é€²è¡Œéæ¡æ¨£ ========== 
# å°‡æ–‡æœ¬ç·¨ç¢¼çš„ "input_ids" è½‰æ›ç‚º NumPy é™£åˆ—é€²è¡Œ SMOTE
input_ids_np = encoded_texts['input_ids'].numpy()

# ä½¿ç”¨ SMOTE é€²è¡Œéæ¡æ¨£è™•ç†
smote = SMOTE(sampling_strategy=1, random_state=42)
input_ids_resampled, labels_resampled = smote.fit_resample(input_ids_np, np.array(labels))

# ========== 5. æ›´æ–° Attention Mask ========== 
# é‡æ–°å‰µå»ºä¸€å€‹èˆ‡éæ¡æ¨£å¾Œ input_ids ç›¸å°æ‡‰çš„ attention_mask
attention_mask_resampled = np.zeros_like(input_ids_resampled)  # å…ˆå‰µå»ºç©ºçš„ attention_mask
for i in range(len(input_ids_resampled)):
    # ç‚ºæ¯å€‹éæ¡æ¨£çš„æ¨£æœ¬ç”Ÿæˆ attention_maskï¼Œæ ¹æ“šåŸå§‹çš„ input_ids çš„ padding å¡«å……æƒ…æ³
    attention_mask_resampled[i] = encoded_texts['attention_mask'][i % len(encoded_texts['attention_mask'])].numpy()

# è½‰å› PyTorch å¼µé‡æ ¼å¼
input_ids_resampled = torch.tensor(input_ids_resampled)
attention_mask_resampled = torch.tensor(attention_mask_resampled)

# æ›´æ–° DataLoader
class FraudDataset(Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.labels[idx],
        }

# å‰µå»ºéæ¡æ¨£å¾Œçš„æ•¸æ“šé›†å’Œæ•¸æ“šåŠ è¼‰å™¨
# æ›´æ–°çš„éƒ¨åˆ†ï¼šç¢ºä¿ labels è½‰æ›ç‚º LongTensor é¡å‹
dataset_resampled = FraudDataset(input_ids_resampled, attention_mask_resampled, torch.tensor(labels_resampled, dtype=torch.long))
dataloader_resampled = DataLoader(dataset_resampled, batch_size=8, shuffle=True)

# ========== 6. åŠ è¼‰ BERT æ¨¡å‹ ========== 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained("bert-base-chinese", num_labels=2).to(device)

# ========== 7. è¨­å®šå„ªåŒ–å™¨èˆ‡æå¤±å‡½æ•¸ ========== 
optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = CrossEntropyLoss()

# ========== 8. è¨“ç·´æ¨¡å‹ ========== 
def train(model, dataloader, optimizer, loss_fn, epochs=2):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            # æ‰“å°æ¨¡å‹çš„è¼¸å‡ºå’Œæå¤±
            # print(f"Logits: {logits.shape}, Labels: {labels.shape}")
            
            loss = loss_fn(logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
        
        print(f"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}")

train(model, dataloader_resampled, optimizer, loss_fn)

# ==========  å„²å­˜æ¨¡å‹èˆ‡ tokenizer ========== 

# å„²å­˜æ¨¡å‹
model.save_pretrained(model_save_path)

# å„²å­˜ tokenizer
tokenizer.save_pretrained(model_save_path)

print(f"æ¨¡å‹å’Œ tokenizer å·²ä¿å­˜åˆ° {model_save_path} è³‡æ–™å¤¾ã€‚")


# ========== 9. è©é¨™æª¢æ¸¬å‡½æ•¸ ========== 
def predict(text, model, tokenizer):
    model.eval()
    encoded = encode_texts([text], tokenizer)
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
    
    logits = outputs.logits
    probs = F.softmax(logits, dim=-1)
    
    fraud_prob = probs[0][1].item() * 100  # è©é¨™çš„æ©Ÿç‡
    return fraud_prob

# ========== 10. æ¸¬è©¦è©é¨™è¾¨è­˜ ========== 
test_texts = [
    "é€™æ˜¯ç©©è³ºä¸è³ çš„æŠ•è³‡æ©Ÿæœƒï¼Œå¿«ä¾†åŠ å…¥ï¼",
    "æ‚¨çš„ä¿¡ç”¨å¡æœ‰ç•°å¸¸äº¤æ˜“ï¼Œè«‹é»æ“Šä»¥ä¸‹é€£çµç¢ºèªã€‚",
    "ä»Šå¤©çš„å¤©æ°£å¾ˆå¥½ï¼Œé©åˆå‡ºå»æ•£æ­¥ã€‚",
    "æ‚¨æœ‰æœªæ”¯ä»˜çš„å¸³å–®ï¼Œè«‹ç«‹å³æ”¯ä»˜ä»¥å…å½±éŸ¿ä¿¡ç”¨ã€‚",
    "æˆ‘å€‘çš„ç¶²ç«™æ­£åœ¨é€²è¡Œç¶­è­·ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
    "æ‚¨çš„è¨‚å–®å·²ç¶“æˆåŠŸè™•ç†ï¼Œè«‹è€å¿ƒç­‰å¾…é…é€ã€‚",
    "å’•åš•",
    "æ‚¨çš„è³¬æˆ¶æ­£åœ¨è¢«ç•°å¸¸ç™»éŒ„ï¼Œè«‹ç«‹å³ä¿®æ”¹å¯†ç¢¼ã€‚",
    "è¦ä¸è¦å‡ºå»ç©",
    "æ„Ÿè¬æ‚¨çš„è¨‚è³¼ï¼Œæˆ‘å€‘å·²ç¶“é–‹å§‹è™•ç†æ‚¨çš„è¨‚å–®ã€‚",
    "ä½ å¥½çƒçƒ",
    "4486",
    "ä»Šå¤©ä¸Šèª²çš„è€å¸«å‡ºçš„ä½œæ¥­å¾ˆå¤šï¼Œæ©Ÿè»Šï¼Œä½œæ¥­å¾ˆå¤šä¸æƒ³å¯«",
    "éº¥ç•¶å‹å…¬å¸æ˜¯ä¸€å®¶ç¾åœ‹è·¨åœ‹é€Ÿé£Ÿé€£é–ä¼æ¥­ï¼Œæ–¼1940å¹´ç”±ç†æŸ¥å’Œè«é‡Œæ–¯Â·éº¥ç•¶å‹å…„å¼Ÿåœ¨ç¾åœ‹åŠ åˆ©ç¦å°¼äºå·è–è²ç´è¿ªè«¾å¸‚æˆç«‹ï¼Œç•¶æ™‚æ˜¯ä¸€å®¶é¤å»³ã€‚ä»–å€‘å°‡è‡ªå·±çš„æ¥­å‹™é‡æ–°å‘½åç‚ºæ¼¢å ¡æ”¤ï¼Œå¾Œä¾†å°‡å…¬å¸è½‰è®Šç‚ºåŠ ç›Ÿé€£é–ä¼æ¥­ï¼Œä¸¦æ–¼1953å¹´åœ¨äºåˆ©æ¡‘é‚£å·é³³å‡°åŸçš„ä¸€å€‹åœ°é»æ¨å‡ºäº†é‡‘è‰²æ‹±é–€æ¨™èªŒã€‚1955å¹´ï¼Œå•†äººé›·Â·å…‹æ´›å…‹ä½œç‚ºåŠ ç›Ÿé€£é–ä»£ç†åŠ å…¥å…¬å¸ï¼Œä¸¦æ–¼1961å¹´è²·ä¸‹éº¥ç•¶å‹å…„å¼Ÿçš„å…¨éƒ¨è‚¡ä»½ã€‚ç¸½éƒ¨å…ˆå‰ä½æ–¼ä¼Šåˆ©è«¾å·å¥§å…‹å¸ƒé­¯å…‹ï¼Œæ–¼2018å¹´6æœˆé·è‡³èŠåŠ å“¥ã€‚éº¥ç•¶å‹ä¹Ÿæ˜¯ä¸€å®¶æˆ¿åœ°ç”¢å…¬å¸ï¼Œæ“æœ‰ç´„70%çš„é¤å»³å»ºç¯‰ç­‰ï¼ˆå‡ºç§Ÿçµ¦å…¶åŠ ç›Ÿé€£é–å•†ï¼‰ã€‚"
]

for text in test_texts:
    prob = predict(text, model, tokenizer)
    print(f"ã€{text}ã€‘è©é¨™å¯èƒ½æ€§: {prob:.2f}%")
